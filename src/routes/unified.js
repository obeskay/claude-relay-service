const express = require('express')
const { authenticateApiKey } = require('../middleware/auth')
const logger = require('../utils/logger')
const { handleChatCompletion } = require('./openaiClaudeRoutes')
// ‰ªé handlers/geminiHandlers.js ÂØºÂÖ•ProcesarFunci√≥n
const {
  handleGenerateContent: geminiHandleGenerateContent,
  handleStreamGenerateContent: geminiHandleStreamGenerateContent
} = require('../handlers/geminiHandlers')
const openaiRoutes = require('./openaiRoutes')
const apiKeyService = require('../services/apiKeyService')

const router = express.Router()

// üîç Ê†πÊçÆÊ®°ÂûãNombreÊ£ÄÊµãÂêéÁ´ØTipo
function detectBackendFromModel(modelName) {
  if (!modelName) {
    return 'claude' // Predeterminado Claude
  }

  const model = modelName.toLowerCase()

  // Claude Ê®°Âûã
  if (model.startsWith('claude-')) {
    return 'claude'
  }

  // Gemini Ê®°Âûã
  if (model.startsWith('gemini-')) {
    return 'gemini'
  }

  // OpenAI Ê®°Âûã
  if (model.startsWith('gpt-')) {
    return 'openai'
  }

  // Predeterminado‰ΩøÁî® Claude
  return 'claude'
}

// üöÄ Êô∫ËÉΩÂêéÁ´ØRutaProcesarÂô®
async function routeToBackend(req, res, requestedModel) {
  const backend = detectBackendFromModel(requestedModel)

  logger.info(`üîÄ Routing request - Model: ${requestedModel}, Backend: ${backend}`)

  // VerificarPermiso
  const { permissions } = req.apiKey

  if (backend === 'claude') {
    // Claude ÂêéÁ´ØÔºöÈÄöËøá OpenAI ÂÖºÂÆπÂ±Ç
    if (!apiKeyService.hasPermission(permissions, 'claude')) {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access Claude',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }
    await handleChatCompletion(req, res, req.apiKey)
  } else if (backend === 'openai') {
    // OpenAI ÂêéÁ´Ø
    if (!apiKeyService.hasPermission(permissions, 'openai')) {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access OpenAI',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }
    return await openaiRoutes.handleResponses(req, res)
  } else if (backend === 'gemini') {
    // Gemini ÂêéÁ´Ø
    if (!apiKeyService.hasPermission(permissions, 'gemini')) {
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access Gemini',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }

    // Convertir‰∏∫ Gemini Formato
    const geminiRequest = {
      model: requestedModel,
      messages: req.body.messages,
      temperature: req.body.temperature || 0.7,
      max_tokens: req.body.max_tokens || 4096,
      stream: req.body.stream || false
    }

    req.body = geminiRequest

    if (geminiRequest.stream) {
      return await geminiHandleStreamGenerateContent(req, res)
    } else {
      return await geminiHandleGenerateContent(req, res)
    }
  } else {
    return res.status(500).json({
      error: {
        message: `Unsupported backend: ${backend}`,
        type: 'server_error',
        code: 'unsupported_backend'
      }
    })
  }
}

// üîÑ OpenAI ÂÖºÂÆπÁöÑ chat/completions EndpointÔºàÊô∫ËÉΩÂêéÁ´ØRutaÔºâ
router.post('/v1/chat/completions', authenticateApiKey, async (req, res) => {
  try {
    // ValidarRequeridoPar√°metro
    if (!req.body.messages || !Array.isArray(req.body.messages) || req.body.messages.length === 0) {
      return res.status(400).json({
        error: {
          message: 'Messages array is required and cannot be empty',
          type: 'invalid_request_error',
          code: 'invalid_request'
        }
      })
    }

    const requestedModel = req.body.model || 'claude-3-5-sonnet-20241022'
    req.body.model = requestedModel // Á°Æ‰øùÊ®°ÂûãÂ∑≤Establecer

    // ‰ΩøÁî®Áªü‰∏ÄÁöÑÂêéÁ´ØRutaProcesarÂô®
    await routeToBackend(req, res, requestedModel)
  } catch (error) {
    logger.error('‚ùå OpenAI chat/completions error:', error)
    if (!res.headersSent) {
      res.status(500).json({
        error: {
          message: 'Internal server error',
          type: 'server_error',
          code: 'internal_error'
        }
      })
    }
  }
})

// üîÑ OpenAI ÂÖºÂÆπÁöÑ completions EndpointÔºà‰º†ÁªüFormatoÔºåÊô∫ËÉΩÂêéÁ´ØRutaÔºâ
router.post('/v1/completions', authenticateApiKey, async (req, res) => {
  try {
    // ValidarRequeridoPar√°metro
    if (!req.body.prompt) {
      return res.status(400).json({
        error: {
          message: 'Prompt is required',
          type: 'invalid_request_error',
          code: 'invalid_request'
        }
      })
    }

    // Â∞Ü‰º†Áªü completions FormatoConvertir‰∏∫ chat Formato
    const originalBody = req.body
    const requestedModel = originalBody.model || 'claude-3-5-sonnet-20241022'

    req.body = {
      model: requestedModel,
      messages: [
        {
          role: 'user',
          content: originalBody.prompt
        }
      ],
      max_tokens: originalBody.max_tokens,
      temperature: originalBody.temperature,
      top_p: originalBody.top_p,
      stream: originalBody.stream,
      stop: originalBody.stop,
      n: originalBody.n || 1,
      presence_penalty: originalBody.presence_penalty,
      frequency_penalty: originalBody.frequency_penalty,
      logit_bias: originalBody.logit_bias,
      user: originalBody.user
    }

    // ‰ΩøÁî®Áªü‰∏ÄÁöÑÂêéÁ´ØRutaProcesarÂô®
    await routeToBackend(req, res, requestedModel)
  } catch (error) {
    logger.error('‚ùå OpenAI completions error:', error)
    if (!res.headersSent) {
      res.status(500).json({
        error: {
          message: 'Failed to process completion request',
          type: 'server_error',
          code: 'internal_error'
        }
      })
    }
  }
})

module.exports = router
module.exports.detectBackendFromModel = detectBackendFromModel
module.exports.routeToBackend = routeToBackend
