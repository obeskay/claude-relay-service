const express = require('express')
const axios = require('axios')
const router = express.Router()
const logger = require('../utils/logger')
const config = require('../../config/config')
const { authenticateApiKey } = require('../middleware/auth')
const unifiedOpenAIScheduler = require('../services/scheduler/unifiedOpenAIScheduler')
const openaiAccountService = require('../services/account/openaiAccountService')
const openaiResponsesAccountService = require('../services/account/openaiResponsesAccountService')
const openaiResponsesRelayService = require('../services/relay/openaiResponsesRelayService')
const apiKeyService = require('../services/apiKeyService')
const redis = require('../models/redis')
const crypto = require('crypto')
const ProxyHelper = require('../utils/proxyHelper')
const { updateRateLimitCounters } = require('../utils/rateLimitHelper')
const { IncrementalSSEParser } = require('../utils/sseParser')
const { getSafeMessage } = require('../utils/errorSanitizer')

// CrearProxy Agentï¼ˆä½¿ç”¨ç»Ÿä¸€çš„Proxyå·¥å…·ï¼‰
function createProxyAgent(proxy) {
  return ProxyHelper.createProxyAgent(proxy)
}

// Verificar API Key æ˜¯å¦å…·å¤‡ OpenAI Permiso
function checkOpenAIPermissions(apiKeyData) {
  return apiKeyService.hasPermission(apiKeyData?.permissions, 'openai')
}

function normalizeHeaders(headers = {}) {
  if (!headers || typeof headers !== 'object') {
    return {}
  }
  const normalized = {}
  for (const [key, value] of Object.entries(headers)) {
    if (!key) {
      continue
    }
    normalized[key.toLowerCase()] = Array.isArray(value) ? value[0] : value
  }
  return normalized
}

function toNumberSafe(value) {
  if (value === undefined || value === null || value === '') {
    return null
  }
  const num = Number(value)
  return Number.isFinite(num) ? num : null
}

function extractCodexUsageHeaders(headers) {
  const normalized = normalizeHeaders(headers)
  if (!normalized || Object.keys(normalized).length === 0) {
    return null
  }

  const snapshot = {
    primaryUsedPercent: toNumberSafe(normalized['x-codex-primary-used-percent']),
    primaryResetAfterSeconds: toNumberSafe(normalized['x-codex-primary-reset-after-seconds']),
    primaryWindowMinutes: toNumberSafe(normalized['x-codex-primary-window-minutes']),
    secondaryUsedPercent: toNumberSafe(normalized['x-codex-secondary-used-percent']),
    secondaryResetAfterSeconds: toNumberSafe(normalized['x-codex-secondary-reset-after-seconds']),
    secondaryWindowMinutes: toNumberSafe(normalized['x-codex-secondary-window-minutes']),
    primaryOverSecondaryPercent: toNumberSafe(
      normalized['x-codex-primary-over-secondary-limit-percent']
    )
  }

  const hasData = Object.values(snapshot).some((value) => value !== null)
  return hasData ? snapshot : null
}

async function applyRateLimitTracking(req, usageSummary, model, context = '', accountType = null) {
  if (!req.rateLimitInfo) {
    return
  }

  const label = context ? ` (${context})` : ''

  try {
    const { totalTokens, totalCost } = await updateRateLimitCounters(
      req.rateLimitInfo,
      usageSummary,
      model,
      req.apiKey?.id,
      accountType
    )

    if (totalTokens > 0) {
      logger.api(`ðŸ“Š Updated rate limit token count${label}: +${totalTokens} tokens`)
    }
    if (typeof totalCost === 'number' && totalCost > 0) {
      logger.api(`ðŸ’° Updated rate limit cost count${label}: +$${totalCost.toFixed(6)}`)
    }
  } catch (error) {
    logger.error(`âŒ Failed to update rate limit counters${label}:`, error)
  }
}

// ä½¿ç”¨ç»Ÿä¸€è°ƒåº¦å™¨é€‰æ‹© OpenAI Cuenta
async function getOpenAIAuthToken(apiKeyData, sessionId = null, requestedModel = null) {
  try {
    // GenerarSesiÃ³nå“ˆå¸Œï¼ˆå¦‚æžœæœ‰SesiÃ³nIDï¼‰
    const sessionHash = sessionId
      ? crypto.createHash('sha256').update(sessionId).digest('hex')
      : null

    // ä½¿ç”¨ç»Ÿä¸€è°ƒåº¦å™¨é€‰æ‹©Cuenta
    const result = await unifiedOpenAIScheduler.selectAccountForApiKey(
      apiKeyData,
      sessionHash,
      requestedModel
    )

    if (!result || !result.accountId) {
      const error = new Error('No available OpenAI account found')
      error.statusCode = 402 // Payment Required - èµ„æºè€—å°½
      throw error
    }

    // æ ¹æ®CuentaTipoObtenerCuentaè¯¦æƒ…
    let account,
      accessToken,
      proxy = null

    if (result.accountType === 'openai-responses') {
      // Procesar OpenAI-Responses Cuenta
      account = await openaiResponsesAccountService.getAccount(result.accountId)
      if (!account || !account.apiKey) {
        const error = new Error(`OpenAI-Responses account ${result.accountId} has no valid apiKey`)
        error.statusCode = 403 // Forbidden - CuentaConfiguraciÃ³nError
        throw error
      }

      // OpenAI-Responses Cuentaä¸éœ€è¦ accessTokenï¼Œç›´æŽ¥RetornarCuentaInformaciÃ³n
      accessToken = null // OpenAI-Responses ä½¿ç”¨Cuentaå†…çš„ apiKey

      // AnalizarProxyConfiguraciÃ³n
      if (account.proxy) {
        try {
          proxy = typeof account.proxy === 'string' ? JSON.parse(account.proxy) : account.proxy
        } catch (e) {
          logger.warn('Failed to parse proxy configuration:', e)
        }
      }

      logger.info(`Selected OpenAI-Responses account: ${account.name} (${result.accountId})`)
    } else {
      // Procesaræ™®é€š OpenAI Cuenta
      account = await openaiAccountService.getAccount(result.accountId)
      if (!account || !account.accessToken) {
        const error = new Error(`OpenAI account ${result.accountId} has no valid accessToken`)
        error.statusCode = 403 // Forbidden - CuentaConfiguraciÃ³nError
        throw error
      }

      // Verificar token æ˜¯å¦è¿‡æœŸå¹¶è‡ªåŠ¨åˆ·æ–°ï¼ˆåŒé‡ä¿æŠ¤ï¼‰
      if (openaiAccountService.isTokenExpired(account)) {
        if (account.refreshToken) {
          logger.info(`ðŸ”„ Token expired, auto-refreshing for account ${account.name} (fallback)`)
          try {
            await openaiAccountService.refreshAccountToken(result.accountId)
            // é‡æ–°ObtenerActualizaråŽçš„Cuenta
            account = await openaiAccountService.getAccount(result.accountId)
            logger.info(`âœ… Token refreshed successfully in route handler`)
          } catch (refreshError) {
            logger.error(`Failed to refresh token for ${account.name}:`, refreshError)
            const error = new Error(`Token expired and refresh failed: ${refreshError.message}`)
            error.statusCode = 403 // Forbidden - è®¤è¯FallÃ³
            throw error
          }
        } else {
          const error = new Error(
            `Token expired and no refresh token available for account ${account.name}`
          )
          error.statusCode = 403 // Forbidden - è®¤è¯FallÃ³
          throw error
        }
      }

      // Descifrado accessTokenï¼ˆaccount.accessToken æ˜¯Cifradoçš„ï¼‰
      accessToken = openaiAccountService.decrypt(account.accessToken)
      if (!accessToken) {
        const error = new Error('Failed to decrypt OpenAI accessToken')
        error.statusCode = 403 // Forbidden - ConfiguraciÃ³n/PermisoError
        throw error
      }

      // AnalizarProxyConfiguraciÃ³n
      if (account.proxy) {
        try {
          proxy = typeof account.proxy === 'string' ? JSON.parse(account.proxy) : account.proxy
        } catch (e) {
          logger.warn('Failed to parse proxy configuration:', e)
        }
      }

      logger.info(`Selected OpenAI account: ${account.name} (${result.accountId})`)
    }

    return {
      accessToken,
      accountId: result.accountId,
      accountName: account.name,
      accountType: result.accountType,
      proxy,
      account
    }
  } catch (error) {
    logger.error('Failed to get OpenAI auth token:', error)
    throw error
  }
}

// ä¸»ProcesarFunciÃ³nï¼Œä¾›ä¸¤ä¸ªRutaå…±äº«
const handleResponses = async (req, res) => {
  let upstream = null
  let accountId = null
  let accountType = 'openai'
  let sessionHash = null
  let account = null
  let proxy = null
  let accessToken = null

  try {
    // ä»ŽMiddlewareObtener API Key Datos
    const apiKeyData = req.apiKey || {}

    if (!checkOpenAIPermissions(apiKeyData)) {
      logger.security(
        `ðŸš« API Key ${apiKeyData.id || 'unknown'} ç¼ºå°‘ OpenAI Permisoï¼Œæ‹’ç»è®¿é—® ${req.originalUrl}`
      )
      return res.status(403).json({
        error: {
          message: 'This API key does not have permission to access OpenAI',
          type: 'permission_denied',
          code: 'permission_denied'
        }
      })
    }

    // ä»ŽSolicitudå¤´æˆ–Solicitudä½“ä¸­æå–SesiÃ³n ID
    const sessionId =
      req.headers['session_id'] ||
      req.headers['x-session-id'] ||
      req.body?.session_id ||
      req.body?.conversation_id ||
      null

    sessionHash = sessionId ? crypto.createHash('sha256').update(sessionId).digest('hex') : null

    // ä»ŽSolicitudä½“ä¸­æå–æ¨¡åž‹å’Œæµå¼æ ‡å¿—
    let requestedModel = req.body?.model || null
    const isCodexModel =
      typeof requestedModel === 'string' && requestedModel.toLowerCase().includes('codex')

    // å¦‚æžœæ¨¡åž‹æ˜¯ gpt-5 å¼€å¤´ä¸”åŽé¢è¿˜æœ‰å†…å®¹ï¼ˆå¦‚ gpt-5-2025-08-07ï¼‰ï¼Œå¹¶ä¸”ä¸æ˜¯ Codex ç³»Columnaï¼Œåˆ™è¦†ç›–ä¸º gpt-5
    if (requestedModel && requestedModel.startsWith('gpt-5-') && !isCodexModel) {
      logger.info(`ðŸ“ Model ${requestedModel} detected, normalizing to gpt-5 for Codex API`)
      requestedModel = 'gpt-5'
      req.body.model = 'gpt-5' // åŒæ—¶ActualizarSolicitudä½“ä¸­çš„æ¨¡åž‹
    }

    const isStream = req.body?.stream !== false // Predeterminadoä¸ºæµå¼ï¼ˆå…¼å®¹çŽ°æœ‰Filaä¸ºï¼‰

    // åˆ¤æ–­æ˜¯å¦ä¸º Codex CLI çš„Solicitudï¼ˆåŸºäºŽ User-Agentï¼‰
    // Soportar: codex_vscode, codex_cli_rs, codex_exec (éžäº¤äº’å¼/è„šæœ¬æ¨¡å¼)
    const userAgent = req.headers['user-agent'] || ''
    const codexCliPattern = /^(codex_vscode|codex_cli_rs|codex_exec)\/[\d.]+/i
    const isCodexCLI = codexCliPattern.test(userAgent)

    // å¦‚æžœä¸æ˜¯ Codex CLI Solicitudï¼Œåˆ™è¿›Filaé€‚é…
    if (!isCodexCLI) {
      // EliminaciÃ³nä¸éœ€è¦çš„Solicitudä½“Campo
      const fieldsToRemove = [
        'temperature',
        'top_p',
        'max_output_tokens',
        'user',
        'text_formatting',
        'truncation',
        'text',
        'service_tier',
        'prompt_cache_retention',
        'safety_identifier'
      ]
      fieldsToRemove.forEach((field) => {
        delete req.body[field]
      })

      // Establecerå›ºå®šçš„ Codex CLI instructions
      req.body.instructions =
        "You are Codex, based on GPT-5. You are running as a coding agent in the Codex CLI on a user's computer.\n\n## General\n\n- When searching for text or files, prefer using `rg` or `rg --files` respectively because `rg` is much faster than alternatives like `grep`. (If the `rg` command is not found, then use alternatives.)\n\n## Editing constraints\n\n- Default to ASCII when editing or creating files. Only introduce non-ASCII or other Unicode characters when there is a clear justification and the file already uses them.\n- Add succinct code comments that explain what is going on if code is not self-explanatory. You should not add comments like \"Assigns the value to the variable\", but a brief comment might be useful ahead of a complex code block that the user would otherwise have to spend time parsing out. Usage of these comments should be rare.\n- Try to use apply_patch for single file edits, but it is fine to explore other options to make the edit if it does not work well. Do not use apply_patch for changes that are auto-generated (i.e. generating package.json or running a lint or format command like gofmt) or when scripting is more efficient (such as search and replacing a string across a codebase).\n- You may be in a dirty git worktree.\n    * NEVER revert existing changes you did not make unless explicitly requested, since these changes were made by the user.\n    * If asked to make a commit or code edits and there are unrelated changes to your work or changes that you didn't make in those files, don't revert those changes.\n    * If the changes are in files you've touched recently, you should read carefully and understand how you can work with the changes rather than reverting them.\n    * If the changes are in unrelated files, just ignore them and don't revert them.\n- Do not amend a commit unless explicitly requested to do so.\n- While you are working, you might notice unexpected changes that you didn't make. If this happens, STOP IMMEDIATELY and ask the user how they would like to proceed.\n- **NEVER** use destructive commands like `git reset --hard` or `git checkout --` unless specifically requested or approved by the user.\n\n## Plan tool\n\nWhen using the planning tool:\n- Skip using the planning tool for straightforward tasks (roughly the easiest 25%).\n- Do not make single-step plans.\n- When you made a plan, update it after having performed one of the sub-tasks that you shared on the plan.\n\n## Codex CLI harness, sandboxing, and approvals\n\nThe Codex CLI harness supports several different configurations for sandboxing and escalation approvals that the user can choose from.\n\nFilesystem sandboxing defines which files can be read or written. The options for `sandbox_mode` are:\n- **read-only**: The sandbox only permits reading files.\n- **workspace-write**: The sandbox permits reading files, and editing files in `cwd` and `writable_roots`. Editing files in other directories requires approval.\n- **danger-full-access**: No filesystem sandboxing - all commands are permitted.\n\nNetwork sandboxing defines whether network can be accessed without approval. Options for `network_access` are:\n- **restricted**: Requires approval\n- **enabled**: No approval needed\n\nApprovals are your mechanism to get user consent to run shell commands without the sandbox. Possible configuration options for `approval_policy` are\n- **untrusted**: The harness will escalate most commands for user approval, apart from a limited allowlist of safe \"read\" commands.\n- **on-failure**: The harness will allow all commands to run in the sandbox (if enabled), and failures will be escalated to the user for approval to run again without the sandbox.\n- **on-request**: Commands will be run in the sandbox by default, and you can specify in your tool call if you want to escalate a command to run without sandboxing. (Note that this mode is not always available. If it is, you'll see parameters for it in the `shell` command description.)\n- **never**: This is a non-interactive mode where you may NEVER ask the user for approval to run commands. Instead, you must always persist and work around constraints to solve the task for the user. You MUST do your utmost best to finish the task and validate your work before yielding. If this mode is paired with `danger-full-access`, take advantage of it to deliver the best outcome for the user. Further, in this mode, your default testing philosophy is overridden: Even if you don't see local patterns for testing, you may add tests and scripts to validate your work. Just remove them before yielding.\n\nWhen you are running with `approval_policy == on-request`, and sandboxing enabled, here are scenarios where you'll need to request approval:\n- You need to run a command that writes to a directory that requires it (e.g. running tests that write to /var)\n- You need to run a GUI app (e.g., open/xdg-open/osascript) to open browsers or files.\n- You are running sandboxed and need to run a command that requires network access (e.g. installing packages)\n- If you run a command that is important to solving the user's query, but it fails because of sandboxing, rerun the command with approval. ALWAYS proceed to use the `with_escalated_permissions` and `justification` parameters - do not message the user before requesting approval for the command.\n- You are about to take a potentially destructive action such as an `rm` or `git reset` that the user did not explicitly ask for\n- (for all of these, you should weigh alternative paths that do not require approval)\n\nWhen `sandbox_mode` is set to read-only, you'll need to request approval for any command that isn't a read.\n\nYou will be told what filesystem sandboxing, network sandboxing, and approval mode are active in a developer or user message. If you are not told about this, assume that you are running with workspace-write, network sandboxing enabled, and approval on-failure.\n\nAlthough they introduce friction to the user because your work is paused until the user responds, you should leverage them when necessary to accomplish important work. If the completing the task requires escalated permissions, Do not let these settings or the sandbox deter you from attempting to accomplish the user's task unless it is set to \"never\", in which case never ask for approvals.\n\nWhen requesting approval to execute a command that will require escalated privileges:\n  - Provide the `with_escalated_permissions` parameter with the boolean value true\n  - Include a short, 1 sentence explanation for why you need to enable `with_escalated_permissions` in the justification parameter\n\n## Special user requests\n\n- If the user makes a simple request (such as asking for the time) which you can fulfill by running a terminal command (such as `date`), you should do so.\n- If the user asks for a \"review\", default to a code review mindset: prioritise identifying bugs, risks, behavioural regressions, and missing tests. Findings must be the primary focus of the response - keep summaries or overviews brief and only after enumerating the issues. Present findings first (ordered by severity with file/line references), follow with open questions or assumptions, and offer a change-summary only as a secondary detail. If no findings are discovered, state that explicitly and mention any residual risks or testing gaps.\n\n## Frontend tasks\nWhen doing frontend design tasks, avoid collapsing into \"AI slop\" or safe, average-looking layouts.\nAim for interfaces that feel intentional, bold, and a bit surprising.\n- Typography: Use expressive, purposeful fonts and avoid default stacks (Inter, Roboto, Arial, system).\n- Color & Look: Choose a clear visual direction; define CSS variables; avoid purple-on-white defaults. No purple bias or dark mode bias.\n- Motion: Use a few meaningful animations (page-load, staggered reveals) instead of generic micro-motions.\n- Background: Don't rely on flat, single-color backgrounds; use gradients, shapes, or subtle patterns to build atmosphere.\n- Overall: Avoid boilerplate layouts and interchangeable UI patterns. Vary themes, type families, and visual languages across outputs.\n- Ensure the page loads properly on both desktop and mobile\n\nException: If working within an existing website or design system, preserve the established patterns, structure, and visual language.\n\n## Presenting your work and final message\n\nYou are producing plain text that will later be styled by the CLI. Follow these rules exactly. Formatting should make results easy to scan, but not feel mechanical. Use judgment to decide how much structure adds value.\n\n- Default: be very concise; friendly coding teammate tone.\n- Ask only when needed; suggest ideas; mirror the user's style.\n- For substantial work, summarize clearly; follow finalâ€‘answer formatting.\n- Skip heavy formatting for simple confirmations.\n- Don't dump large files you've written; reference paths only.\n- No \"save/copy this file\" - User is on the same machine.\n- Offer logical next steps (tests, commits, build) briefly; add verify steps if you couldn't do something.\n- For code changes:\n  * Lead with a quick explanation of the change, and then give more details on the context covering where and why a change was made. Do not start this explanation with \"summary\", just jump right in.\n  * If there are natural next steps the user may want to take, suggest them at the end of your response. Do not make suggestions if there are no natural next steps.\n  * When suggesting multiple options, use numeric lists for the suggestions so the user can quickly respond with a single number.\n- The user does not command execution outputs. When asked to show the output of a command (e.g. `git show`), relay the important details in your answer or summarize the key lines so the user understands the result.\n\n### Final answer structure and style guidelines\n\n- Plain text; CLI handles styling. Use structure only when it helps scanability.\n- Headers: optional; short Title Case (1-3 words) wrapped in **â€¦**; no blank line before the first bullet; add only if they truly help.\n- Bullets: use - ; merge related points; keep to one line when possible; 4â€“6 per list ordered by importance; keep phrasing consistent.\n- Monospace: backticks for commands/paths/env vars/code ids and inline examples; use for literal keyword bullets; never combine with **.\n- Code samples or multi-line snippets should be wrapped in fenced code blocks; include an info string as often as possible.\n- Structure: group related bullets; order sections general â†’ specific â†’ supporting; for subsections, start with a bolded keyword bullet, then items; match complexity to the task.\n- Tone: collaborative, concise, factual; present tense, active voice; selfâ€‘contained; no \"above/below\"; parallel wording.\n- Don'ts: no nested bullets/hierarchies; no ANSI codes; don't cram unrelated keywords; keep keyword lists shortâ€”wrap/reformat if long; avoid naming formatting styles in answers.\n- Adaptation: code explanations â†’ precise, structured with code refs; simple tasks â†’ lead with outcome; big changes â†’ logical walkthrough + rationale + next actions; casual one-offs â†’ plain sentences, no headers/bullets.\n- File References: When referencing files in your response follow the below rules:\n  * Use inline code to make file paths clickable.\n  * Each reference should have a stand alone path. Even if it's the same file.\n  * Accepted: absolute, workspaceâ€‘relative, a/ or b/ diff prefixes, or bare filename/suffix.\n  * Optionally include line/column (1â€‘based): :line[:column] or #Lline[Ccolumn] (column defaults to 1).\n  * Do not use URIs like file://, vscode://, or https://.\n  * Do not provide range of lines\n  * Examples: src/app.ts, src/app.ts:42, b/server/index.js#L10, C:\\repo\\project\\main.rs:12:5\n"

      logger.info('ðŸ“ Non-Codex CLI request detected, applying Codex CLI adaptation')
    } else {
      logger.info('âœ… Codex CLI request detected, forwarding as-is')
    }

    // ä½¿ç”¨è°ƒåº¦å™¨é€‰æ‹©Cuenta
    ;({ accessToken, accountId, accountType, proxy, account } = await getOpenAIAuthToken(
      apiKeyData,
      sessionId,
      requestedModel
    ))

    // å¦‚æžœæ˜¯ OpenAI-Responses Cuentaï¼Œä½¿ç”¨ä¸“é—¨çš„ä¸­ç»§ServicioProcesar
    if (accountType === 'openai-responses') {
      logger.info(`ðŸ”€ Using OpenAI-Responses relay service for account: ${account.name}`)
      return await openaiResponsesRelayService.handleRequest(req, res, account, apiKeyData)
    }
    // åŸºäºŽç™½åå•æž„é€ ä¸Šæ¸¸æ‰€éœ€çš„Solicitudå¤´ï¼Œç¡®ä¿é”®ä¸ºå°å†™ä¸”Valorå—æŽ§
    const incoming = req.headers || {}

    const allowedKeys = ['version', 'openai-beta', 'session_id']

    const headers = {}
    for (const key of allowedKeys) {
      if (incoming[key] !== undefined) {
        headers[key] = incoming[key]
      }
    }

    // åˆ¤æ–­æ˜¯å¦è®¿é—® compact Endpoint
    const isCompactRoute =
      req.path === '/responses/compact' ||
      req.path === '/v1/responses/compact' ||
      (req.originalUrl && req.originalUrl.includes('/responses/compact'))

    // è¦†ç›–æˆ–Nueva caracterÃ­sticaå¿…è¦å¤´éƒ¨
    headers['authorization'] = `Bearer ${accessToken}`
    headers['chatgpt-account-id'] = account.accountId || account.chatgptUserId || accountId
    headers['host'] = 'chatgpt.com'
    headers['accept'] = isStream ? 'text/event-stream' : 'application/json'
    headers['content-type'] = 'application/json'
    if (!isCompactRoute) {
      req.body['store'] = false
    } else if (req.body && Object.prototype.hasOwnProperty.call(req.body, 'store')) {
      delete req.body['store']
    }

    // CrearProxy agent
    const proxyAgent = createProxyAgent(proxy)

    // ConfiguraciÃ³nSolicitudé€‰é¡¹
    const axiosConfig = {
      headers,
      timeout: config.requestTimeout || 600000,
      validateStatus: () => true
    }

    // å¦‚æžœæœ‰Proxyï¼Œæ·»åŠ ProxyConfiguraciÃ³n
    if (proxyAgent) {
      axiosConfig.httpAgent = proxyAgent
      axiosConfig.httpsAgent = proxyAgent
      axiosConfig.proxy = false
      logger.info(`ðŸŒ Using proxy for OpenAI request: ${ProxyHelper.getProxyDescription(proxy)}`)
    } else {
      logger.debug('ðŸŒ No proxy configured for OpenAI request')
    }

    const codexEndpoint = isCompactRoute
      ? 'https://chatgpt.com/backend-api/codex/responses/compact'
      : 'https://chatgpt.com/backend-api/codex/responses'

    // æ ¹æ® stream ParÃ¡metroå†³å®šSolicitudTipo
    if (isStream) {
      // æµå¼Solicitud
      upstream = await axios.post(codexEndpoint, req.body, {
        ...axiosConfig,
        responseType: 'stream'
      })
    } else {
      // éžæµå¼Solicitud
      upstream = await axios.post(codexEndpoint, req.body, axiosConfig)
    }

    const codexUsageSnapshot = extractCodexUsageHeaders(upstream.headers)
    if (codexUsageSnapshot) {
      try {
        await openaiAccountService.updateCodexUsageSnapshot(accountId, codexUsageSnapshot)
      } catch (codexError) {
        logger.error('âš ï¸ Actualizar Codex ä½¿ç”¨EstadÃ­sticaFallÃ³:', codexError)
      }
    }

    // Procesar 429 é™æµError
    if (upstream.status === 429) {
      logger.warn(`ðŸš« Rate limit detected for OpenAI account ${accountId} (Codex API)`)

      // AnalizarRespuestaä½“ä¸­çš„é™æµInformaciÃ³n
      let resetsInSeconds = null
      let errorData = null

      try {
        // å¯¹äºŽ429Errorï¼Œæ— è®ºæ˜¯å¦æ˜¯æµå¼Solicitudï¼ŒRespuestaéƒ½ä¼šæ˜¯å®Œæ•´çš„JSONErrorObjeto
        if (isStream && upstream.data) {
          // æµå¼Respuestaéœ€è¦å…ˆæ”¶é›†Datos
          const chunks = []
          await new Promise((resolve, reject) => {
            upstream.data.on('data', (chunk) => chunks.push(chunk))
            upstream.data.on('end', resolve)
            upstream.data.on('error', reject)
            // EstablecerTiempo de espera agotadoé˜²æ­¢æ— é™ç­‰å¾…
            setTimeout(resolve, 5000)
          })

          const fullResponse = Buffer.concat(chunks).toString()
          try {
            errorData = JSON.parse(fullResponse)
          } catch (e) {
            logger.error('Failed to parse 429 error response:', e)
            logger.debug('Raw response:', fullResponse)
          }
        } else {
          // éžæµå¼Respuestaç›´æŽ¥ä½¿ç”¨data
          errorData = upstream.data
        }

        // æå–é‡ç½®Tiempo
        if (errorData && errorData.error && errorData.error.resets_in_seconds) {
          resetsInSeconds = errorData.error.resets_in_seconds
          logger.info(
            `ðŸ• Codex rate limit will reset in ${resetsInSeconds} seconds (${Math.ceil(resetsInSeconds / 60)} minutes / ${Math.ceil(resetsInSeconds / 3600)} hours)`
          )
        } else {
          logger.warn(
            'âš ï¸ Could not extract resets_in_seconds from 429 response, using default 60 minutes'
          )
        }
      } catch (e) {
        logger.error('âš ï¸ Failed to parse rate limit error:', e)
      }

      // æ ‡è®°Cuentaä¸ºé™æµçŠ¶æ€
      await unifiedOpenAIScheduler.markAccountRateLimited(
        accountId,
        'openai',
        sessionHash,
        resetsInSeconds
      )

      // RetornarErrorRespuestaç»™Cliente
      const errorResponse = errorData || {
        error: {
          type: 'usage_limit_reached',
          message: 'The usage limit has been reached',
          resets_in_seconds: resetsInSeconds
        }
      }

      if (isStream) {
        // æµå¼Respuestaä¹Ÿéœ€è¦Estableceræ­£ç¡®çš„çŠ¶æ€ç 
        res.status(429)
        res.setHeader('Content-Type', 'text/event-stream')
        res.setHeader('Cache-Control', 'no-cache')
        res.setHeader('Connection', 'keep-alive')
        res.write(`data: ${JSON.stringify(errorResponse)}\n\n`)
        res.end()
      } else {
        res.status(429).json(errorResponse)
      }

      return
    } else if (upstream.status === 401 || upstream.status === 402) {
      const unauthorizedStatus = upstream.status
      const statusDescription = unauthorizedStatus === 401 ? 'Unauthorized' : 'Payment required'
      logger.warn(
        `ðŸ” ${statusDescription} error detected for OpenAI account ${accountId} (Codex API)`
      )

      let errorData = null

      try {
        if (isStream && upstream.data && typeof upstream.data.on === 'function') {
          const chunks = []
          await new Promise((resolve, reject) => {
            upstream.data.on('data', (chunk) => chunks.push(chunk))
            upstream.data.on('end', resolve)
            upstream.data.on('error', reject)
            setTimeout(resolve, 5000)
          })

          const fullResponse = Buffer.concat(chunks).toString()
          try {
            errorData = JSON.parse(fullResponse)
          } catch (parseError) {
            logger.error(`Failed to parse ${unauthorizedStatus} error response:`, parseError)
            logger.debug(`Raw ${unauthorizedStatus} response:`, fullResponse)
            errorData = { error: { message: fullResponse || 'Unauthorized' } }
          }
        } else {
          errorData = upstream.data
        }
      } catch (parseError) {
        logger.error(`âš ï¸ Failed to handle ${unauthorizedStatus} error response:`, parseError)
      }

      const statusLabel = unauthorizedStatus === 401 ? '401Error' : '402Error'
      const extraHint = unauthorizedStatus === 402 ? 'ï¼Œå¯èƒ½æ¬ è´¹' : ''
      let reason = `OpenAIè´¦å·è®¤è¯FallÃ³ï¼ˆ${statusLabel}${extraHint}ï¼‰`
      if (errorData) {
        const messageCandidate =
          errorData.error &&
          typeof errorData.error.message === 'string' &&
          errorData.error.message.trim()
            ? errorData.error.message.trim()
            : typeof errorData.message === 'string' && errorData.message.trim()
              ? errorData.message.trim()
              : null
        if (messageCandidate) {
          reason = `OpenAIè´¦å·è®¤è¯FallÃ³ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${messageCandidate}`
        }
      }

      try {
        await unifiedOpenAIScheduler.markAccountUnauthorized(
          accountId,
          'openai',
          sessionHash,
          reason
        )
      } catch (markError) {
        logger.error(
          `âŒ Failed to mark OpenAI account unauthorized after ${unauthorizedStatus}:`,
          markError
        )
      }

      let errorResponse = errorData
      if (!errorResponse || typeof errorResponse !== 'object' || Buffer.isBuffer(errorResponse)) {
        const fallbackMessage =
          typeof errorData === 'string' && errorData.trim() ? errorData.trim() : 'Unauthorized'
        errorResponse = {
          error: {
            message: fallbackMessage,
            type: 'unauthorized',
            code: 'unauthorized'
          }
        }
      }

      res.status(unauthorizedStatus).json(errorResponse)
      return
    } else if (upstream.status === 200 || upstream.status === 201) {
      // SolicitudÃ‰xitoï¼ŒVerificarå¹¶EliminaciÃ³né™æµçŠ¶æ€
      const isRateLimited = await unifiedOpenAIScheduler.isAccountRateLimited(accountId)
      if (isRateLimited) {
        logger.info(
          `âœ… Removing rate limit for OpenAI account ${accountId} after successful request`
        )
        await unifiedOpenAIScheduler.removeAccountRateLimit(accountId, 'openai')
      }
    }

    res.status(upstream.status)

    if (isStream) {
      // æµå¼Respuestaå¤´
      res.setHeader('Content-Type', 'text/event-stream')
      res.setHeader('Cache-Control', 'no-cache')
      res.setHeader('Connection', 'keep-alive')
      res.setHeader('X-Accel-Buffering', 'no')
    } else {
      // éžæµå¼Respuestaå¤´
      res.setHeader('Content-Type', 'application/json')
    }

    // é€ä¼ å…³é”®è¯Šæ–­å¤´ï¼Œé¿å…ä¼ é€’ä¸Seguridadæˆ–ä¸Žä¼ è¾“ç›¸å…³çš„å¤´
    const passThroughHeaderKeys = ['openai-version', 'x-request-id', 'openai-processing-ms']
    for (const key of passThroughHeaderKeys) {
      const val = upstream.headers?.[key]
      if (val !== undefined) {
        res.setHeader(key, val)
      }
    }

    if (isStream) {
      // ç«‹å³åˆ·æ–°Respuestaå¤´ï¼ŒIniciando SSE
      if (typeof res.flushHeaders === 'function') {
        res.flushHeaders()
      }
    }

    // ProcesarRespuestaå¹¶æ•èŽ· usage Datoså’ŒçœŸå®žçš„ model
    let usageData = null
    let actualModel = null
    let usageReported = false
    let rateLimitDetected = false
    let rateLimitResetsInSeconds = null

    if (!isStream) {
      // éžæµå¼RespuestaProcesar
      try {
        logger.info(`ðŸ“„ Processing OpenAI non-stream response for model: ${requestedModel}`)

        // ç›´æŽ¥Obtenerå®Œæ•´Respuesta
        const responseData = upstream.data

        // ä»ŽRespuestaä¸­Obtenerå®žé™…çš„ model å’Œ usage
        actualModel = responseData.model || requestedModel || 'gpt-4'
        usageData = responseData.usage

        logger.debug(`ðŸ“Š Non-stream response - Model: ${actualModel}, Usage:`, usageData)

        // Registroä½¿ç”¨EstadÃ­stica
        if (usageData) {
          const totalInputTokens = usageData.input_tokens || usageData.prompt_tokens || 0
          const outputTokens = usageData.output_tokens || usageData.completion_tokens || 0
          const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0
          // Calcularå®žé™…è¾“å…¥tokenï¼ˆæ€»è¾“å…¥å‡åŽ»CachÃ©éƒ¨åˆ†ï¼‰
          const actualInputTokens = Math.max(0, totalInputTokens - cacheReadTokens)

          await apiKeyService.recordUsage(
            apiKeyData.id,
            actualInputTokens, // ä¼ é€’å®žé™…è¾“å…¥ï¼ˆä¸å«CachÃ©ï¼‰
            outputTokens,
            0, // OpenAIæ²¡æœ‰cache_creation_tokens
            cacheReadTokens,
            actualModel,
            accountId,
            'openai'
          )

          logger.info(
            `ðŸ“Š Recorded OpenAI non-stream usage - Input: ${totalInputTokens}(actual:${actualInputTokens}+cached:${cacheReadTokens}), Output: ${outputTokens}, Total: ${usageData.total_tokens || totalInputTokens + outputTokens}, Model: ${actualModel}`
          )

          await applyRateLimitTracking(
            req,
            {
              inputTokens: actualInputTokens,
              outputTokens,
              cacheCreateTokens: 0,
              cacheReadTokens
            },
            actualModel,
            'openai-non-stream',
            'openai'
          )
        }

        // RetornarRespuesta
        res.json(responseData)
        return
      } catch (error) {
        logger.error('Failed to process non-stream response:', error)
        if (!res.headersSent) {
          res.status(500).json({ error: { message: 'Failed to process response' } })
        }
        return
      }
    }

    // ä½¿ç”¨å¢žé‡ SSE Analizarå™¨
    const sseParser = new IncrementalSSEParser()

    // ProcesarAnalizarå‡ºçš„Evento
    const processSSEEvent = (eventData) => {
      // Verificaræ˜¯å¦æ˜¯ response.completed Evento
      if (eventData.type === 'response.completed' && eventData.response) {
        // ä»ŽRespuestaä¸­ObtenerçœŸå®žçš„ model
        if (eventData.response.model) {
          actualModel = eventData.response.model
          logger.debug(`ðŸ“Š Captured actual model: ${actualModel}`)
        }

        // Obtener usage Datos
        if (eventData.response.usage) {
          usageData = eventData.response.usage
          logger.debug('ðŸ“Š Captured OpenAI usage data:', usageData)
        }
      }

      // Verificaræ˜¯å¦æœ‰é™æµError
      if (eventData.error && eventData.error.type === 'usage_limit_reached') {
        rateLimitDetected = true
        if (eventData.error.resets_in_seconds) {
          rateLimitResetsInSeconds = eventData.error.resets_in_seconds
          logger.warn(
            `ðŸš« Rate limit detected in stream, resets in ${rateLimitResetsInSeconds} seconds`
          )
        }
      }
    }

    upstream.data.on('data', (chunk) => {
      try {
        // è½¬å‘Datosç»™Cliente
        if (!res.destroyed) {
          res.write(chunk)
        }

        // ä½¿ç”¨å¢žé‡Analizarå™¨ProcesarDatos
        const events = sseParser.feed(chunk.toString())
        for (const event of events) {
          if (event.type === 'data' && event.data) {
            processSSEEvent(event.data)
          }
        }
      } catch (error) {
        logger.error('Error processing OpenAI stream chunk:', error)
      }
    })

    upstream.data.on('end', async () => {
      // Procesarå‰©ä½™çš„ buffer
      const remaining = sseParser.getRemaining()
      if (remaining.trim()) {
        const events = sseParser.feed('\n\n') // å¼ºåˆ¶åˆ·æ–°å‰©ä½™å†…å®¹
        for (const event of events) {
          if (event.type === 'data' && event.data) {
            processSSEEvent(event.data)
          }
        }
      }

      // Registroä½¿ç”¨EstadÃ­stica
      if (!usageReported && usageData) {
        try {
          const totalInputTokens = usageData.input_tokens || 0
          const outputTokens = usageData.output_tokens || 0
          const cacheReadTokens = usageData.input_tokens_details?.cached_tokens || 0
          // Calcularå®žé™…è¾“å…¥tokenï¼ˆæ€»è¾“å…¥å‡åŽ»CachÃ©éƒ¨åˆ†ï¼‰
          const actualInputTokens = Math.max(0, totalInputTokens - cacheReadTokens)

          // ä½¿ç”¨Respuestaä¸­çš„çœŸå®ž modelï¼Œå¦‚æžœæ²¡æœ‰åˆ™ä½¿ç”¨Solicitudä¸­çš„ modelï¼Œæœ€åŽRetiradaåˆ°PredeterminadoValor
          const modelToRecord = actualModel || requestedModel || 'gpt-4'

          await apiKeyService.recordUsage(
            apiKeyData.id,
            actualInputTokens, // ä¼ é€’å®žé™…è¾“å…¥ï¼ˆä¸å«CachÃ©ï¼‰
            outputTokens,
            0, // OpenAIæ²¡æœ‰cache_creation_tokens
            cacheReadTokens,
            modelToRecord,
            accountId,
            'openai'
          )

          logger.info(
            `ðŸ“Š Recorded OpenAI usage - Input: ${totalInputTokens}(actual:${actualInputTokens}+cached:${cacheReadTokens}), Output: ${outputTokens}, Total: ${usageData.total_tokens || totalInputTokens + outputTokens}, Model: ${modelToRecord} (actual: ${actualModel}, requested: ${requestedModel})`
          )
          usageReported = true

          await applyRateLimitTracking(
            req,
            {
              inputTokens: actualInputTokens,
              outputTokens,
              cacheCreateTokens: 0,
              cacheReadTokens
            },
            modelToRecord,
            'openai-stream',
            'openai'
          )
        } catch (error) {
          logger.error('Failed to record OpenAI usage:', error)
        }
      }

      // å¦‚æžœåœ¨æµå¼Respuestaä¸­æ£€æµ‹åˆ°é™æµ
      if (rateLimitDetected) {
        logger.warn(`ðŸš« Processing rate limit for OpenAI account ${accountId} from stream`)
        await unifiedOpenAIScheduler.markAccountRateLimited(
          accountId,
          'openai',
          sessionHash,
          rateLimitResetsInSeconds
        )
      } else if (upstream.status === 200) {
        // æµå¼SolicitudÃ‰xitoï¼ŒVerificarå¹¶EliminaciÃ³né™æµçŠ¶æ€
        const isRateLimited = await unifiedOpenAIScheduler.isAccountRateLimited(accountId)
        if (isRateLimited) {
          logger.info(
            `âœ… Removing rate limit for OpenAI account ${accountId} after successful stream`
          )
          await unifiedOpenAIScheduler.removeAccountRateLimit(accountId, 'openai')
        }
      }

      res.end()
    })

    upstream.data.on('error', (err) => {
      logger.error('Upstream stream error:', err)
      if (!res.headersSent) {
        res.status(502).json({ error: { message: 'Upstream stream error' } })
      } else {
        res.end()
      }
    })

    // Clienteæ–­å¼€æ—¶Limpiarä¸Šæ¸¸æµ
    const cleanup = () => {
      try {
        upstream.data?.unpipe?.(res)
        upstream.data?.destroy?.()
      } catch (_) {
        //
      }
    }
    req.on('close', cleanup)
    req.on('aborted', cleanup)
  } catch (error) {
    logger.error('Proxy to ChatGPT codex/responses failed:', error)
    // ä¼˜å…ˆä½¿ç”¨ä¸»åŠ¨Establecerçš„ statusCodeï¼Œç„¶åŽæ˜¯ä¸Šæ¸¸Respuestaçš„çŠ¶æ€ç ï¼Œæœ€åŽPredeterminado 500
    const status = error.statusCode || error.response?.status || 500

    if ((status === 401 || status === 402) && accountId) {
      const statusLabel = status === 401 ? '401Error' : '402Error'
      const extraHint = status === 402 ? 'ï¼Œå¯èƒ½æ¬ è´¹' : ''
      let reason = `OpenAIè´¦å·è®¤è¯FallÃ³ï¼ˆ${statusLabel}${extraHint}ï¼‰`
      const errorData = error.response?.data
      if (errorData) {
        if (typeof errorData === 'string' && errorData.trim()) {
          reason = `OpenAIè´¦å·è®¤è¯FallÃ³ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${errorData.trim()}`
        } else if (
          errorData.error &&
          typeof errorData.error.message === 'string' &&
          errorData.error.message.trim()
        ) {
          reason = `OpenAIè´¦å·è®¤è¯FallÃ³ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${errorData.error.message.trim()}`
        } else if (typeof errorData.message === 'string' && errorData.message.trim()) {
          reason = `OpenAIè´¦å·è®¤è¯FallÃ³ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${errorData.message.trim()}`
        }
      } else if (error.message) {
        reason = `OpenAIè´¦å·è®¤è¯FallÃ³ï¼ˆ${statusLabel}${extraHint}ï¼‰ï¼š${error.message}`
      }

      try {
        await unifiedOpenAIScheduler.markAccountUnauthorized(
          accountId,
          accountType || 'openai',
          sessionHash,
          reason
        )
      } catch (markError) {
        logger.error('âŒ Failed to mark OpenAI account unauthorized in catch handler:', markError)
      }
    }

    let responsePayload = error.response?.data
    if (!responsePayload) {
      responsePayload = { error: { message: getSafeMessage(error) } }
    } else if (typeof responsePayload === 'string') {
      responsePayload = { error: { message: getSafeMessage(responsePayload) } }
    } else if (typeof responsePayload === 'object' && !responsePayload.error) {
      responsePayload = {
        error: { message: getSafeMessage(responsePayload.message || error) }
      }
    } else if (responsePayload.error?.message) {
      responsePayload.error.message = getSafeMessage(responsePayload.error.message)
    }

    if (!res.headersSent) {
      res.status(status).json(responsePayload)
    }
  }
}

// æ³¨å†Œä¸¤ä¸ªRutaRutaï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„ProcesarFunciÃ³n
router.post('/responses', authenticateApiKey, handleResponses)
router.post('/v1/responses', authenticateApiKey, handleResponses)
router.post('/responses/compact', authenticateApiKey, handleResponses)
router.post('/v1/responses/compact', authenticateApiKey, handleResponses)

// ä½¿ç”¨æƒ…å†µEstadÃ­sticaEndpoint
router.get('/usage', authenticateApiKey, async (req, res) => {
  try {
    const keyData = req.apiKey
    // æŒ‰éœ€Consulta usage Datos
    const usage = await redis.getUsageStats(keyData.id)

    res.json({
      object: 'usage',
      total_tokens: usage?.total?.tokens || 0,
      total_requests: usage?.total?.requests || 0,
      daily_tokens: usage?.daily?.tokens || 0,
      daily_requests: usage?.daily?.requests || 0,
      monthly_tokens: usage?.monthly?.tokens || 0,
      monthly_requests: usage?.monthly?.requests || 0
    })
  } catch (error) {
    logger.error('Failed to get usage stats:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve usage statistics',
        type: 'api_error'
      }
    })
  }
})

// API Key InformaciÃ³nEndpoint
router.get('/key-info', authenticateApiKey, async (req, res) => {
  try {
    const keyData = req.apiKey
    // æŒ‰éœ€Consulta usage Datosï¼ˆä»… key-info Endpointéœ€è¦ï¼‰
    const usage = await redis.getUsageStats(keyData.id)
    const tokensUsed = usage?.total?.tokens || 0
    res.json({
      id: keyData.id,
      name: keyData.name,
      description: keyData.description,
      permissions: keyData.permissions,
      token_limit: keyData.tokenLimit,
      tokens_used: tokensUsed,
      tokens_remaining:
        keyData.tokenLimit > 0 ? Math.max(0, keyData.tokenLimit - tokensUsed) : null,
      rate_limit: {
        window: keyData.rateLimitWindow,
        requests: keyData.rateLimitRequests
      },
      usage: {
        total: usage?.total || {},
        daily: usage?.daily || {},
        monthly: usage?.monthly || {}
      }
    })
  } catch (error) {
    logger.error('Failed to get key info:', error)
    res.status(500).json({
      error: {
        message: 'Failed to retrieve API key information',
        type: 'api_error'
      }
    })
  }
})

module.exports = router
module.exports.handleResponses = handleResponses
